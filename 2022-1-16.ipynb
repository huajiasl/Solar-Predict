{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据导入\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "dataset = pd.read_csv(r'E:\\Datasets-2020.11.6\\5.0-Site_31-TDG.csv')\n",
    "\n",
    "dataset = dataset.rename(columns={\n",
    "    u'31 TDG - Active Energy Delivered-Received (kWh)': 'AE_Power',\n",
    "    u'31 TDG - Current Phase Average (A)': 'Current', #电流\n",
    "    u'31 TDG - Active Power (kW)': 'Power',   #功率\n",
    "    u'31 TDG - Performance Ratio (%)': 'PR',   #性能比\n",
    "    u'31 TDG - Wind Speed (m/s)': 'WindSpeed',    #风速\n",
    "    u'31 TDG - Weather Temperature': 'Temp', #气温\n",
    "    u'31 TDG - Weather Relative Humidity (%)': 'Humidaty',   #相对湿度\n",
    "    u'31 TDG - Global Horizontal Radiation': 'GHI',   #全球水平辐照度\n",
    "    u'31 TDG - Diffuse Horizontal Radiation': 'DHI',  #扩散水平辐射\n",
    "    u'31 TDG - Wind Direction (Degrees)': 'WD',  #风向\n",
    "    u'31 TDG - Weather Daily Rainfall (mm)':'RF',#降雨\n",
    "})\n",
    "\n",
    "dataset = dataset.drop(['AE_Power','Current','PR','WindSpeed','WD','RF'],axis=1)#删除列,直接是列名\n",
    "\n",
    "dataset = dataset.dropna(subset=['Power'])# 删除功率为空的数据组\n",
    "\n",
    "dataset = dataset.fillna(0) # 所有NAN值赋0\n",
    "\n",
    "dataset = dataset.drop(dataset[dataset.Power < 0].index)\n",
    "dataset = dataset.drop(dataset[(dataset.Temp > 2000)|(dataset.Temp < 0)].index)\n",
    "dataset = dataset.drop(dataset[dataset.Humidaty < 0].index)\n",
    "dataset = dataset.drop(dataset[(dataset.GHI > 2000)|(dataset.GHI < 0)].index)\n",
    "dataset = dataset.drop(dataset[dataset.DHI < 0].index)\n",
    "\n",
    "df14 = pd.read_csv(r'E:\\Datasets-2020.11.6\\uv-alice-springs-2014.csv')\n",
    "df15 = pd.read_csv(r'E:\\Datasets-2020.11.6\\uv-alice-springs-2015.csv')\n",
    "df16 = pd.read_csv(r'E:\\Datasets-2020.11.6\\uv-alice-springs-2016.csv')\n",
    "df17 = pd.read_csv(r'E:\\Datasets-2020.11.6\\uv-alice-springs-2017.csv')\n",
    "df18 = pd.read_csv(r'E:\\Datasets-2020.11.6\\uv-alice-springs-2018.csv')\n",
    "df19 = pd.read_csv(r'E:\\Datasets-2020.11.6\\uv-alice-springs-2019.csv')\n",
    "\n",
    "df14['timestamp'] = pd.to_datetime(df14['timestamp'])\n",
    "df15['timestamp'] = pd.to_datetime(df15['timestamp'])\n",
    "df16['timestamp'] = pd.to_datetime(df16['timestamp'])\n",
    "df17['timestamp'] = pd.to_datetime(df17['timestamp'])\n",
    "df18['timestamp'] = pd.to_datetime(df18['timestamp'])\n",
    "df19['timestamp'] = pd.to_datetime(df19['timestamp'])\n",
    "dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])\n",
    "\n",
    "df = pd.concat([df14,df15,df16,df17,df18,df19],axis=0) #axis=0表示按行操作\n",
    "df = df.drop(['Lat','Lon'],axis=1)\n",
    "\n",
    "dataset_n = pd.merge(dataset,df,how='left',on='timestamp')\n",
    "\n",
    "dataset_n = dataset_n.dropna(subset=['UV_Index'])\n",
    "dataset_n = dataset_n.drop(dataset_n[dataset_n.UV_Index < 0].index)\n",
    "\n",
    "minibatch=288\n",
    "\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "x_valid=[]\n",
    "y_valid=[]\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "\n",
    "training_set = dataset_n.iloc[0:518688,1:7] #1441x288 # 360x288 415008:518688\n",
    "test_set = dataset_n.iloc[568750:576526,1:7] # 27x288\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "test_set_scaled=sc.transform(test_set)\n",
    "\n",
    "np.random.seed(6)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(6)\n",
    "np.random.shuffle(y_train)\n",
    "tf.random.set_seed(6)\n",
    "\n",
    "x_train = tf.cast(training_set_scaled[0:415008,1:6], tf.float32)\n",
    "y_train = tf.cast(training_set_scaled[0:415008,0:1], tf.float32)\n",
    "x_valid = tf.cast(training_set_scaled[415008:518689,1:6], tf.float32)\n",
    "y_valid = tf.cast(training_set_scaled[415008:518689,0:1], tf.float32)\n",
    "x_test = tf.cast(test_set_scaled[:,1:6], tf.float32)\n",
    "y_test = tf.cast(test_set_scaled[:,0:1], tf.float32)\n",
    "\n",
    "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(minibatch)\n",
    "valid_db = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(minibatch)\n",
    "\n",
    "No_layer = 3 #层数3\n",
    "layer_0_N=5 #输入层神经元5\n",
    "layer_1_N=5 #隐藏层神经元5\n",
    "layer_2_N=5 #隐藏层神经元5\n",
    "layer_3_N=1 #输出层神经元1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数初始化\n",
    "w1 = tf.Variable(tf.random.normal([layer_0_N, layer_1_N],mean=0, stddev=1))\n",
    "b1 = tf.Variable(tf.random.normal([layer_1_N],mean=1, stddev=1))\n",
    "w2 = tf.Variable(tf.random.normal([layer_1_N, layer_2_N], mean=0, stddev=1))\n",
    "b2 = tf.Variable(tf.random.normal([layer_2_N], mean=1, stddev=1))\n",
    "w3 = tf.Variable(tf.random.normal([layer_2_N, layer_3_N],mean=0, stddev=1))\n",
    "b3 = tf.Variable(tf.random.normal([layer_3_N], mean=1, stddev=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr =  0.1 # 学习率为0.1\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
    "epoch = 20  # 循环50轮\n",
    "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和\n",
    "loss_all_v = 0\n",
    "m_w1,m_w2,m_w22,m_w3,m_b1,m_b2,m_b22,m_b3 = 0,0,0,0,0,0,0,0\n",
    "beta = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 199.71041605994105,loss_valid:0.1386561095714569\n",
      "Epoch 1, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 2, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 3, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 4, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 5, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 6, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 7, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 8, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 9, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 10, loss: 199.13108322396874,loss_valid:0.1386561095714569\n",
      "Epoch 11, loss: 199.13108322396874,loss_valid:0.1386561095714569\n"
     ]
    }
   ],
   "source": [
    "# 训练部分\n",
    "now_time = time.time()  ##2##\n",
    "for epoch in range(epoch):  # 数据集级别的循环，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  # batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            h1 = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            a1 = tf.nn.relu(h1)  # 第一层激活函数\n",
    "            h2 = tf.matmul(a1,w2) + b2 # 第二层运算\n",
    "            a2 = tf.nn.relu(h2) #第二层激活函数\n",
    "            h3 = tf.matmul(a2,w3) + b3 #第三层运算\n",
    "            a3 = tf.nn.relu(h3) # 第三层激活函数\n",
    "            loss = tf.reduce_mean(tf.square(y_train-a3))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1,w2,w3,b1,b2,b3])\n",
    "\n",
    "        # SGD+M\n",
    "        \n",
    "        m_w1 = beta * m_w1 + (1 - beta) * grads[0]\n",
    "        m_w2 = beta * m_w2 + (1 - beta) * grads[1]\n",
    "        m_w3 = beta * m_w3 + (1 - beta) * grads[2]\n",
    "        m_b1 = beta * m_b1 + (1 - beta) * grads[3]\n",
    "        m_b2 = beta * m_b2 + (1 - beta) * grads[4]\n",
    "        m_b3 = beta * m_b3 + (1 - beta) * grads[5]\n",
    "        \n",
    "        w1.assign_sub(lr * m_w1)  # 参数w1自更新\n",
    "        w2.assign_sub(lr * m_w2)  # 参数w2自更新\n",
    "        w3.assign_sub(lr * m_w3)  # 参数w3自更新\n",
    "        b1.assign_sub(lr * m_b1)  # 参数b1自更新\n",
    "        b2.assign_sub(lr * m_b2)  # 参数b2自更新\n",
    "        b3.assign_sub(lr * m_b3)  # 参数b3自更新\n",
    "    \n",
    "        h1_v = tf.matmul(x_valid, w1) + b1\n",
    "        a1_v = tf.nn.relu(h1_v)\n",
    "        h2_v = tf.matmul(a1_v, w2) + b2\n",
    "        a2_v = tf.nn.relu(h2_v)\n",
    "        h3_v = tf.matmul(a2_v, w3) + b3\n",
    "        a3_v = tf.nn.relu(h3_v)\n",
    "        loss_v = tf.reduce_mean(tf.square(y_valid-a3_v))\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {},loss_valid:{}\".format(epoch, loss_all / (len(x_train)/minibatch), loss_v))\n",
    "    train_loss_results.append(loss_all / (len(x_train)/minibatch))  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
    "print(' train_loss_results=', train_loss_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1,b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_sum=[]\n",
    "\n",
    "p1 = tf.matmul(x_test, w1) + b1\n",
    "q1 = tf.nn.relu(p1)\n",
    "p2 = tf.matmul(q1, w2) + b2\n",
    "q2 = tf.nn.relu(p2)\n",
    "p3 = tf.matmul(q2, w3) + b3\n",
    "q3 = tf.nn.relu(p3)\n",
    "\n",
    "test_set_p = test_set_scaled\n",
    "test_set_p = np.array(test_set_p)\n",
    "test_set_p[:,0:1] = q3\n",
    "p_y_test = sc.inverse_transform(test_set_p)\n",
    "pred=p_y_test[:,0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_pred=[]\n",
    "day_test=[]\n",
    "for n in range(0,7776,288):\n",
    "    day_pred.append(pred[n:n+288].sum())\n",
    "    day_test.append(np.array(test_set)[n:n+288,0:1].sum())\n",
    "print(np.array(day_pred))\n",
    "print(np.array(day_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "ax = plt.axes()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.plot(day_pred,color='red',label='pre power')\n",
    "plt.plot(np.array(day_test),color='blue',label='test power')\n",
    "plt.title('Pre/Test by day',fontsize=28,fontweight='heavy', pad=40)\n",
    "plt.xlabel('Days',fontsize=18)\n",
    "plt.ylabel('Power(kW)',fontsize=18)\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(day_test)-np.array(day_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
